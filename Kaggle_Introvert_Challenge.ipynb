{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "from logging import raiseExceptions"
      ],
      "metadata": {
        "id": "hUobj9AlDVYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pulling Data in from Google Sheets\n",
        "url = \"https://docs.google.com/spreadsheets/d/e/2PACX-1vSTmtBa7FYVX9Y7ALY7lnsQ9j4A3AeeqPRANZqscbNfhU2wtbbCinlHkLOatGZEcscZTcsRdJLHYY17/pubhtml\"\n",
        "html = requests.get(url).text\n",
        "soup = BeautifulSoup(html, \"lxml\")\n",
        "\n",
        "train = []\n",
        "test = []\n",
        "\n",
        "tables = soup.find_all(\"table\")\n",
        "for index, table in enumerate(tables):\n",
        "    for row in table.find_all(\"tr\"):\n",
        "        train.append([cell.text for cell in row.find_all(\"td\")])\n",
        "\n",
        "url = \"https://docs.google.com/spreadsheets/d/e/2PACX-1vSObPeBI-ln_xscvnLARzH11ueaT_YsxPCbYVJF2e1MvmFil7Aq4fbC2eI6u3f0S3xe13VyhmUU1dOi/pubhtml\"\n",
        "html = requests.get(url).text\n",
        "soup = BeautifulSoup(html, \"lxml\")\n",
        "\n",
        "tables = soup.find_all(\"table\")\n",
        "for index, table in enumerate(tables):\n",
        "    for row in table.find_all(\"tr\"):\n",
        "        test.append([cell.text for cell in row.find_all(\"td\")])"
      ],
      "metadata": {
        "id": "dbcRB4IVDVQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DataCleaner(data,isTraining):\n",
        "  output_df = []\n",
        "  y_output = []\n",
        "\n",
        "  # Taking out the ID Variable\n",
        "  for i in range(len(data)):\n",
        "    if(isTraining):\n",
        "      output_df.append(data[i][1:8])\n",
        "      y_output.append(data[i][8])\n",
        "\n",
        "      # Column 0\n",
        "      if output_df[i][0] == '':\n",
        "        output_df[i][0] = -1\n",
        "\n",
        "      # Column 1\n",
        "      if output_df[i][1] == 'Yes':\n",
        "        output_df[i][1] = 1\n",
        "      elif output_df[i][1] == 'No':\n",
        "        output_df[i][1] = 0\n",
        "      elif output_df[i][1] == '':\n",
        "        output_df[i][1] = -1\n",
        "      else:\n",
        "        raiseExceptions\n",
        "\n",
        "      # Column 2\n",
        "      if output_df[i][2] == '':\n",
        "        output_df[i][2] = -1\n",
        "\n",
        "      # Column 3\n",
        "      if output_df[i][3] == '':\n",
        "        output_df[i][3] = -1\n",
        "\n",
        "      # Column 4\n",
        "      if output_df[i][4] == 'Yes':\n",
        "        output_df[i][4] = 1\n",
        "      elif output_df[i][4] == 'No':\n",
        "        output_df[i][4] = 0\n",
        "      elif output_df[i][4] == '':\n",
        "        output_df[i][4] = -1\n",
        "      else:\n",
        "        raiseExceptions\n",
        "\n",
        "      # Column 5\n",
        "      if output_df[i][5] == '':\n",
        "        output_df[i][5] = -1\n",
        "\n",
        "      # Column 6\n",
        "      if output_df[i][6] == '':\n",
        "        output_df[i][6] = -1\n",
        "\n",
        "    else:\n",
        "        output_df.append(data[i][1:])\n",
        "\n",
        "        # Column 0\n",
        "        if output_df[i][0] == '':\n",
        "          output_df[i][0] = -1\n",
        "\n",
        "        # Column 1\n",
        "        if output_df[i][1] == 'Yes':\n",
        "          output_df[i][1] = 1\n",
        "        elif output_df[i][1] == 'No':\n",
        "          output_df[i][1] = 0\n",
        "        elif output_df[i][1] == '':\n",
        "          output_df[i][1] = -1\n",
        "        else:\n",
        "          raiseExceptions\n",
        "\n",
        "        # Column 2\n",
        "        if output_df[i][2] == '':\n",
        "          output_df[i][2] = -1\n",
        "\n",
        "        # Column 3\n",
        "        if output_df[i][3] == '':\n",
        "          output_df[i][3] = -1\n",
        "\n",
        "        # Column 4\n",
        "        if output_df[i][4] == 'Yes':\n",
        "          output_df[i][4] = 1\n",
        "        elif output_df[i][4] == 'No':\n",
        "          output_df[i][4] = 0\n",
        "        elif output_df[i][4] == '':\n",
        "          output_df[i][4] = -1\n",
        "        else:\n",
        "          raiseExceptions\n",
        "\n",
        "        # Column 5\n",
        "        if output_df[i][5] == '':\n",
        "          output_df[i][5] = -1\n",
        "\n",
        "        # Column 6\n",
        "        if output_df[i][6] == '':\n",
        "          output_df[i][6] = -1\n",
        "\n",
        "  if(isTraining):\n",
        "    # Changing the Outcome Variable to Binary\n",
        "    y_output = [1 if x == 'Extrovert' else 0 for x in y_output]\n",
        "\n",
        "    # Returning output and Y values if training data\n",
        "    return output_df, y_output\n",
        "\n",
        "  else:\n",
        "    #Just returning output if not training data\n",
        "    return output_df"
      ],
      "metadata": {
        "id": "D2iOxLQ5ENuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing the Labels from the Data\n",
        "train = train[1:]\n",
        "test = test[1:]\n",
        "\n",
        "# Running the Training and Testing Data through the Data Cleaner\n",
        "train_x, train_y = DataCleaner(train, True)\n",
        "test_x = DataCleaner(test, False)\n",
        "\n",
        "# Converting data to numpy arrays\n",
        "train_x = np.array(train_x)\n",
        "train_x = train_x.astype(float)\n",
        "\n",
        "train_y = np.array(train_y)\n",
        "train_y = train_y.astype(float)\n",
        "\n",
        "test_x = np.array(test_x)\n",
        "test_x = test_x.astype(float)"
      ],
      "metadata": {
        "id": "DOsXCoC-G2s_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc3 = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        return F.log_softmax(self.fc3(x), dim=1)"
      ],
      "metadata": {
        "id": "DizNKwL6HhLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Intializing my Model, Criterion, Optimizer\n",
        "model = SimpleNeuralNet(input_size=train_x.shape[1], num_classes=2)\n",
        "\n",
        "# Calculate class weights\n",
        "class_counts = torch.bincount(torch.tensor(train_y).long())\n",
        "total_samples = len(train_y)\n",
        "class_weights = total_samples / class_counts\n",
        "# Normalize weights (optional, but can be helpful)\n",
        "class_weights = class_weights / class_weights.sum()\n",
        "\n",
        "criterion = nn.NLLLoss(weight=class_weights.float())  # Because we're using log_softmax, apply weights\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Example feature and label tensors\n",
        "X_tensor = torch.tensor(train_x).float()  # Convert to float\n",
        "y_tensor = torch.tensor(train_y)\n",
        "\n",
        "# Create dataset\n",
        "train_dataset = TensorDataset(X_tensor, y_tensor)\n",
        "\n",
        "# Create DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=50, shuffle=True)"
      ],
      "metadata": {
        "id": "40yuU_NMIBs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 1000\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch.long())  # Convert target to Long\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")"
      ],
      "metadata": {
        "id": "qFYQ18ZjRwR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Values with batch_size at 2:\n",
        "# Epoch 1, Loss: 0.1983\n",
        "# Epoch 2, Loss: 0.1780\n",
        "# Epoch 3, Loss: 0.1790\n",
        "# Epoch 4, Loss: 0.1742\n",
        "# Epoch 5, Loss: 0.1731\n",
        "\n",
        "# Values with batch_size at 10:\n",
        "# Epoch 1, Loss: 0.2108\n",
        "# Epoch 2, Loss: 0.1930\n",
        "# Epoch 3, Loss: 0.1880\n",
        "# Epoch 4, Loss: 0.1803\n",
        "# Epoch 5, Loss: 0.1765\n",
        "\n",
        "# Values with batch_size at 25:\n",
        "# Epoch 1, Loss: 0.2134\n",
        "# Epoch 2, Loss: 0.1904\n",
        "# Epoch 3, Loss: 0.1844\n",
        "# Epoch 4, Loss: 0.1822\n",
        "# Epoch 5, Loss: 0.1797\n",
        "\n",
        "# Values with batch_size at 50:\n",
        "# Epoch 1, Loss: 0.2236\n",
        "# Epoch 2, Loss: 0.1933\n",
        "# Epoch 3, Loss: 0.1874\n",
        "# Epoch 4, Loss: 0.1829\n",
        "# Epoch 5, Loss: 0.1812"
      ],
      "metadata": {
        "id": "UmOLDc1fsKI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating my predictions now\n",
        "model.eval()\n",
        "\n",
        "test_tensor = torch.tensor(test_x).float()\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(test_tensor)\n",
        "\n",
        "predicted_classes = torch.argmax(outputs, dim=1)\n",
        "predicted_classes = ['Introvert' if pred == 1 else 'Extrovert' for pred in predicted_classes]\n",
        "\n",
        "# Getting the ids for the predictions\n",
        "ids = []\n",
        "for i in range(len(test)):\n",
        "  ids.append(test[i][0])\n",
        "\n",
        "# Combining the arrays to make a dataset to write to my csv for my submission\n",
        "combined = np.column_stack((ids, predicted_classes))\n",
        "\n",
        "# Convert to Pandas\n",
        "final= pd.DataFrame(combined, columns=['id', 'Personality'])\n",
        "final = final.set_index('id')\n",
        "\n",
        "# Final output\n",
        "final.to_csv(\"submission.csv\")"
      ],
      "metadata": {
        "id": "m07Rh3Sy6eaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nbYddZeiGsYa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
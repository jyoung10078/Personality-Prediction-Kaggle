{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hUobj9AlDVYi"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/Jared.Young/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "from logging import raiseExceptions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dbcRB4IVDVQc"
      },
      "outputs": [
        {
          "ename": "FeatureNotFound",
          "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.google.com/spreadsheets/d/e/2PACX-1vSTmtBa7FYVX9Y7ALY7lnsQ9j4A3AeeqPRANZqscbNfhU2wtbbCinlHkLOatGZEcscZTcsRdJLHYY17/pubhtml\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m html \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m----> 4\u001b[0m soup \u001b[38;5;241m=\u001b[39m \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhtml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlxml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m train \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m test \u001b[38;5;241m=\u001b[39m []\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/bs4/__init__.py:364\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    362\u001b[0m     possible_builder_class \u001b[38;5;241m=\u001b[39m builder_registry\u001b[38;5;241m.\u001b[39mlookup(\u001b[38;5;241m*\u001b[39mfeatures)\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m possible_builder_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m FeatureNotFound(\n\u001b[1;32m    365\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a tree builder with the features you \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    366\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequested: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m. Do you need to install a parser library?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    367\u001b[0m             \u001b[38;5;241m%\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(features)\n\u001b[1;32m    368\u001b[0m         )\n\u001b[1;32m    369\u001b[0m     builder_class \u001b[38;5;241m=\u001b[39m possible_builder_class\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# At this point either we have a TreeBuilder instance in\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# builder, or we have a builder_class that we can instantiate\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m# with the remaining **kwargs.\u001b[39;00m\n",
            "\u001b[0;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
          ]
        }
      ],
      "source": [
        "# Pulling Data in from Google Sheets\n",
        "url = \"https://docs.google.com/spreadsheets/d/e/2PACX-1vSTmtBa7FYVX9Y7ALY7lnsQ9j4A3AeeqPRANZqscbNfhU2wtbbCinlHkLOatGZEcscZTcsRdJLHYY17/pubhtml\"\n",
        "html = requests.get(url).text\n",
        "soup = BeautifulSoup(html, \"lxml\")\n",
        "\n",
        "train = []\n",
        "test = []\n",
        "\n",
        "tables = soup.find_all(\"table\")\n",
        "for index, table in enumerate(tables):\n",
        "    for row in table.find_all(\"tr\"):\n",
        "        train.append([cell.text for cell in row.find_all(\"td\")])\n",
        "\n",
        "url = \"https://docs.google.com/spreadsheets/d/e/2PACX-1vSObPeBI-ln_xscvnLARzH11ueaT_YsxPCbYVJF2e1MvmFil7Aq4fbC2eI6u3f0S3xe13VyhmUU1dOi/pubhtml\"\n",
        "html = requests.get(url).text\n",
        "soup = BeautifulSoup(html, \"lxml\")\n",
        "\n",
        "tables = soup.find_all(\"table\")\n",
        "for index, table in enumerate(tables):\n",
        "    for row in table.find_all(\"tr\"):\n",
        "        test.append([cell.text for cell in row.find_all(\"td\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2iOxLQ5ENuW"
      },
      "outputs": [],
      "source": [
        "def DataCleaner(data,isTraining):\n",
        "  output_df = []\n",
        "  y_output = []\n",
        "\n",
        "  # Taking out the ID Variable\n",
        "  for i in range(len(data)):\n",
        "    if(isTraining):\n",
        "      output_df.append(data[i][1:8])\n",
        "      y_output.append(data[i][8])\n",
        "\n",
        "      # Column 0\n",
        "      if output_df[i][0] == '':\n",
        "        output_df[i][0] = -1\n",
        "\n",
        "      # Column 1\n",
        "      if output_df[i][1] == 'Yes':\n",
        "        output_df[i][1] = 1\n",
        "      elif output_df[i][1] == 'No':\n",
        "        output_df[i][1] = 0\n",
        "      elif output_df[i][1] == '':\n",
        "        output_df[i][1] = -1\n",
        "      else:\n",
        "        raiseExceptions\n",
        "\n",
        "      # Column 2\n",
        "      if output_df[i][2] == '':\n",
        "        output_df[i][2] = -1\n",
        "\n",
        "      # Column 3\n",
        "      if output_df[i][3] == '':\n",
        "        output_df[i][3] = -1\n",
        "\n",
        "      # Column 4\n",
        "      if output_df[i][4] == 'Yes':\n",
        "        output_df[i][4] = 1\n",
        "      elif output_df[i][4] == 'No':\n",
        "        output_df[i][4] = 0\n",
        "      elif output_df[i][4] == '':\n",
        "        output_df[i][4] = -1\n",
        "      else:\n",
        "        raiseExceptions\n",
        "\n",
        "      # Column 5\n",
        "      if output_df[i][5] == '':\n",
        "        output_df[i][5] = -1\n",
        "\n",
        "      # Column 6\n",
        "      if output_df[i][6] == '':\n",
        "        output_df[i][6] = -1\n",
        "\n",
        "    else:\n",
        "        output_df.append(data[i][1:])\n",
        "\n",
        "        # Column 0\n",
        "        if output_df[i][0] == '':\n",
        "          output_df[i][0] = -1\n",
        "\n",
        "        # Column 1\n",
        "        if output_df[i][1] == 'Yes':\n",
        "          output_df[i][1] = 1\n",
        "        elif output_df[i][1] == 'No':\n",
        "          output_df[i][1] = 0\n",
        "        elif output_df[i][1] == '':\n",
        "          output_df[i][1] = -1\n",
        "        else:\n",
        "          raiseExceptions\n",
        "\n",
        "        # Column 2\n",
        "        if output_df[i][2] == '':\n",
        "          output_df[i][2] = -1\n",
        "\n",
        "        # Column 3\n",
        "        if output_df[i][3] == '':\n",
        "          output_df[i][3] = -1\n",
        "\n",
        "        # Column 4\n",
        "        if output_df[i][4] == 'Yes':\n",
        "          output_df[i][4] = 1\n",
        "        elif output_df[i][4] == 'No':\n",
        "          output_df[i][4] = 0\n",
        "        elif output_df[i][4] == '':\n",
        "          output_df[i][4] = -1\n",
        "        else:\n",
        "          raiseExceptions\n",
        "\n",
        "        # Column 5\n",
        "        if output_df[i][5] == '':\n",
        "          output_df[i][5] = -1\n",
        "\n",
        "        # Column 6\n",
        "        if output_df[i][6] == '':\n",
        "          output_df[i][6] = -1\n",
        "\n",
        "  if(isTraining):\n",
        "    # Changing the Outcome Variable to Binary\n",
        "    y_output = [1 if x == 'Extrovert' else 0 for x in y_output]\n",
        "\n",
        "    # Returning output and Y values if training data\n",
        "    return output_df, y_output\n",
        "\n",
        "  else:\n",
        "    #Just returning output if not training data\n",
        "    return output_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOsXCoC-G2s_"
      },
      "outputs": [],
      "source": [
        "# Removing the Labels from the Data\n",
        "train = train[1:]\n",
        "test = test[1:]\n",
        "\n",
        "# Running the Training and Testing Data through the Data Cleaner\n",
        "train_x, train_y = DataCleaner(train, True)\n",
        "test_x = DataCleaner(test, False)\n",
        "\n",
        "# Converting data to numpy arrays\n",
        "train_x = np.array(train_x)\n",
        "train_x = train_x.astype(float)\n",
        "\n",
        "train_y = np.array(train_y)\n",
        "train_y = train_y.astype(float)\n",
        "\n",
        "test_x = np.array(test_x)\n",
        "test_x = test_x.astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DizNKwL6HhLK"
      },
      "outputs": [],
      "source": [
        "class SimpleNeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc3 = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        return F.log_softmax(self.fc3(x), dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40yuU_NMIBs5"
      },
      "outputs": [],
      "source": [
        "# Intializing my Model, Criterion, Optimizer\n",
        "model = SimpleNeuralNet(input_size=train_x.shape[1], num_classes=2)\n",
        "\n",
        "# Calculate class weights\n",
        "class_counts = torch.bincount(torch.tensor(train_y).long())\n",
        "total_samples = len(train_y)\n",
        "class_weights = total_samples / class_counts\n",
        "# Normalize weights (optional, but can be helpful)\n",
        "class_weights = class_weights / class_weights.sum()\n",
        "\n",
        "criterion = nn.NLLLoss(weight=class_weights.float())  # Because we're using log_softmax, apply weights\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Example feature and label tensors\n",
        "X_tensor = torch.tensor(train_x).float()  # Convert to float\n",
        "y_tensor = torch.tensor(train_y)\n",
        "\n",
        "# Create dataset\n",
        "train_dataset = TensorDataset(X_tensor, y_tensor)\n",
        "\n",
        "# Create DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=50, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFYQ18ZjRwR-"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "num_epochs = 1000\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch.long())  # Convert target to Long\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmOLDc1fsKI-"
      },
      "outputs": [],
      "source": [
        "# Values with batch_size at 2:\n",
        "# Epoch 1, Loss: 0.1983\n",
        "# Epoch 2, Loss: 0.1780\n",
        "# Epoch 3, Loss: 0.1790\n",
        "# Epoch 4, Loss: 0.1742\n",
        "# Epoch 5, Loss: 0.1731\n",
        "\n",
        "# Values with batch_size at 10:\n",
        "# Epoch 1, Loss: 0.2108\n",
        "# Epoch 2, Loss: 0.1930\n",
        "# Epoch 3, Loss: 0.1880\n",
        "# Epoch 4, Loss: 0.1803\n",
        "# Epoch 5, Loss: 0.1765\n",
        "\n",
        "# Values with batch_size at 25:\n",
        "# Epoch 1, Loss: 0.2134\n",
        "# Epoch 2, Loss: 0.1904\n",
        "# Epoch 3, Loss: 0.1844\n",
        "# Epoch 4, Loss: 0.1822\n",
        "# Epoch 5, Loss: 0.1797\n",
        "\n",
        "# Values with batch_size at 50:\n",
        "# Epoch 1, Loss: 0.2236\n",
        "# Epoch 2, Loss: 0.1933\n",
        "# Epoch 3, Loss: 0.1874\n",
        "# Epoch 4, Loss: 0.1829\n",
        "# Epoch 5, Loss: 0.1812"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m07Rh3Sy6eaH"
      },
      "outputs": [],
      "source": [
        "# Generating my predictions now\n",
        "model.eval()\n",
        "\n",
        "test_tensor = torch.tensor(test_x).float()\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(test_tensor)\n",
        "\n",
        "predicted_classes = torch.argmax(outputs, dim=1)\n",
        "predicted_classes = ['Introvert' if pred == 1 else 'Extrovert' for pred in predicted_classes]\n",
        "\n",
        "# Getting the ids for the predictions\n",
        "ids = []\n",
        "for i in range(len(test)):\n",
        "  ids.append(test[i][0])\n",
        "\n",
        "# Combining the arrays to make a dataset to write to my csv for my submission\n",
        "combined = np.column_stack((ids, predicted_classes))\n",
        "\n",
        "# Convert to Pandas\n",
        "final= pd.DataFrame(combined, columns=['id', 'Personality'])\n",
        "final = final.set_index('id')\n",
        "\n",
        "# Final output\n",
        "final.to_csv(\"submission.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbYddZeiGsYa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
